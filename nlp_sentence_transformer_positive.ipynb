{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Overview: Identifying Positive Cases and Initial Date of Infection.\n",
    "\n",
    "This notebook was used to process posts from the subset of users self-reporting symptoms of interest [notebook link](nlp_sentence_transformer_self_report.ipynb). This notebook also makes use of the \"sentence_transformers\" module, which is used to load a pretrained BERT ML model for creating text embeddings for sentences. The basic sequence of steps is\n",
    "- Load BERT model\n",
    "- Create dummy phrases for self-reporting positive and convert them to text embeddings using model\n",
    "- Iterate through all relevant Facebook posts/sentences\n",
    "    - Compare dummy sentences to Facebook senteces\n",
    "    - If a match is detected--save the post/user information in a dataset for possible positive users\n",
    "- Manually update a \"date_reported\" field for this data set (i.e. find a date reference within the respective post like \"I got covid last August\")\n",
    "- Save final dataset of self-reporting users with referenced date of infection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained BERT model for phrase/sentence similarity\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# list of \"I tested positive\" phrases\n",
    "covid_synonyms = [\"covid-19\",\"covid19\",\"covid\",\"coronavirus\",\"corona\",\"rona\"]\n",
    "positive_phrases = [\n",
    "    \"I tested positive\",\n",
    "    \"since my <covid> diagnosis\",\n",
    "    \"since my positive diagnosis\",\n",
    "    \"after my positive diagnosis\",\n",
    "    \"ater testing positive\",\n",
    "    \"I had <covid>\",\n",
    "    \"I got <covid>\",\n",
    "    \"diagnosed with <covid>\",\n",
    "    \"it has been since I had <covid>\",\n",
    "]\n",
    "# create list of covid positive phrases using list of commonly used synonyms\n",
    "all_phrases = []\n",
    "for phrase in positive_phrases:\n",
    "    for syn in covid_synonyms:\n",
    "        all_phrases.append(phrase.replace(\"<covid>\",syn))\n",
    "\n",
    "# create text/vector embeddings for each phrase\n",
    "positive_embedding_map = {}\n",
    "for phrase in all_phrases:\n",
    "    embedding = model.encode(phrase, convert_to_tensor=True)\n",
    "    positive_embedding_map[phrase] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data from sqlite DB\n",
    "datadir = r\"C:\\Users\\keatu\\Regis_archive\\practicum_data\"\n",
    "dbfile = os.path.join(datadir,\"Facebook_self_report.db\")\n",
    "con = sqlite3.connect(dbfile)\n",
    "posts = pd.read_sql(\"select * from posts\",con)\n",
    "comments = pd.read_sql(\"select * from comments\", con)\n",
    "replies = pd.read_sql(\"Select * from replies\",con)\n",
    "con.close()\n",
    "\n",
    "# grab text/id fields from each data type--treating them all like unique posts\n",
    "all_text = pd.concat([\n",
    "                    posts[[\"user_id\",\"post_id\",\"text\"]],\n",
    "                    comments[[\"commenter_id\",\"comment_id\",\"comment_text\"]].rename(columns={\"commenter_id\":\"user_id\",\"comment_id\":\"post_id\",\"comment_text\":\"text\"}),\n",
    "                    replies[[\"commenter_id\",\"comment_id\",\"comment_text\"]].rename(columns={\"commenter_id\":\"user_id\",\"comment_id\":\"post_id\",\"comment_text\":\"text\"})\n",
    "                    ], sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare input sentence embedding to all matching sentence combinations and use threshold cosine similarity score\n",
    "positive_report_df = pd.DataFrame()\n",
    "threshold = 0.75 # cosine similarity threshold\n",
    "i=0\n",
    "\n",
    "# iterate through all facebook posts\n",
    "for idx, row in all_text.iterrows():\n",
    "    i=+1\n",
    "    if (i%1000)==0:\n",
    "        print(\"{} completed of {}\".format(i,len(all_text)))\n",
    "    # iterate through each sentence within each post\n",
    "    for sent in sent_tokenize(row['text']):\n",
    "        sent_embed = model.encode(sent, convert_to_tensor=True)\n",
    "        # only select the best match for each input Facebook sentence--this will allow for multiple symptom matches for 1 sentence\n",
    "        top_score = 0\n",
    "        top_match = \"\"\n",
    "        for phrase in positive_embedding_map:\n",
    "            cos_score = util.cos_sim(sent_embed, positive_embedding_map[phrase]).item()\n",
    "            if cos_score > top_score:\n",
    "                top_score = cos_score\n",
    "                top_match = phrase\n",
    "        if top_score > threshold:\n",
    "            positive_report_df = positive_report_df.append({'user_id':row['user_id'],\"post_id\":row['post_id'],'sentence':sent,\"match_sentence\":top_match,\"cos_similarity\":top_score}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe containing these possibly positive users\n",
    "text_with_time = pd.concat([\n",
    "                    posts[[\"user_id\",\"post_id\",\"text\",\"time\"]],\n",
    "                    comments[[\"commenter_id\",\"comment_id\",\"comment_text\",\"comment_time\"]].rename(columns={\"commenter_id\":\"user_id\",\"comment_id\":\"post_id\",\"comment_text\":\"text\",\"comment_time\":\"time\"}),\n",
    "                    replies[[\"commenter_id\",\"comment_id\",\"comment_text\",\"comment_time\"]].rename(columns={\"commenter_id\":\"user_id\",\"comment_id\":\"post_id\",\"comment_text\":\"text\",\"comment_time\":\"time\"})\n",
    "                    ], sort = False)\n",
    "positive_report_with_time = pd.merge(positive_report_df,text_with_time[[\"post_id\",\"time\"]], on=\"post_id\", how=\"left\").sort_values(\"user_id\")\n",
    "\n",
    "# save results to a csv file--this is where I manually entered results\n",
    "positive_report_with_time.to_csv(os.path.join(datadir,\"positive_reporting.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the same file--I added a \"date_reported\" column manually\n",
    "positive_reporting = pd.read_csv(os.path.join(datadir,\"positive_reporting.csv\"))\n",
    "\n",
    "# save all positive reporting users for which a positive infection date can be extracted\n",
    "outcon = sqlite3.connect(r\"C:\\Users\\keatu\\Regis_archive\\practicum_data\\Facebook_Self_Report.db\")\n",
    "positive_reporting[~positive_reporting[\"date_reported\"].isna()].astype(str).to_sql(\"positive_reporting\",con=outcon)\n",
    "outcon.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce4da9915f0d678e12335d58837d25529fa567b369eed2b2f7083145c2fca737"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
