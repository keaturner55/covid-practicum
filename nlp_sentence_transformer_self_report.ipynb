{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Overview: Identify Users Self-Reporting Symptoms of Interest\n",
    "\n",
    "This notebook was used to process all Facebook posts to identify users self-reporting symptoms of interest. This notebook makes use of the \"sentence_transformers\" module, which is used to load a pretrained BERT ML model for creating text embeddings for sentences. The results from this notebook directly apply to the self-reporting positive users notebook [notebook link](nlp_sentence_transformer_positive.ipynb). The basic sequence of steps is\n",
    "- Load BERT model\n",
    "- Create a symptom map between symptom categories and key-words associated to those categories\n",
    "- Create dummy phrases for self-reporting symptoms of interest using this symptom map\n",
    "- Convert dummy phrases to to text embeddings using BERT model\n",
    "- Iterate through all Facebook posts/sentences\n",
    "    - Compare dummy sentences to Facebook senteces\n",
    "    - If a match is detected--save the relevant symptom/user information as an entry\n",
    "- Save final dataset of self-reporting users with referenced date of infection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained BERT model for phrase/sentence similarity\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate map of symptom category (key) and corresponding list of search terms (values)\n",
    "symptom_map = {\n",
    "    \"blood clot\":[\"clot\",\"blood clot\"],\n",
    "    \"heart\":[\"heart problem\",\"heart issue\",\"palpitation\",\"rapid heartbeat\",\"fast heartbeat\", \"increased heart rate\"],\n",
    "    \"stroke\":[\"stroke\"],\n",
    "    \"dvt\":[\"deep vein thrombosis\"],\n",
    "    \"pe\":[\"pulmonary embolism\"],\n",
    "    \"breathing\":[\"out of breath\",\"shortness of breath\",\"trouble breathing\"],\n",
    "    \"lightheaded\":[\"ligthheaded\",\"lightheadedness\",\"faint\",\"dizzy\",\"vertigo\"],\n",
    "    \"leg\":[\"leg pain\",\"leg swelling\"],\n",
    "    \"skin\":[\"clammy skin\",\"skin discoloration\",\"cyanosis\"],\n",
    "}\n",
    "# generate list of common self-report symptom phrases\n",
    "self_report_phrases = [\"I had <symptom>\",\"I experienced <symptom>\",\"I felt <symptom>\",\"I suffer from\",\n",
    "\"my symptoms included <symptom>\",\"I felt <symptom>\",\"dealing with <symptom>\",\"has anyone else <symptom>\"]\n",
    "\n",
    "# create map between each search term (key) and the resultant self-report phrases and corresponding text embeddings\n",
    "symptom_embedding_map = {}\n",
    "for symptom_key in symptom_map:\n",
    "    symptom_embedding_map[symptom_key] = []\n",
    "    for symptom in symptom_map[symptom_key]:\n",
    "        for phrase in self_report_phrases:\n",
    "            phrase = phrase.replace(\"<symptom>\",symptom)\n",
    "            embedding = model.encode(phrase, convert_to_tensor=True)\n",
    "            symptom_embedding_map[symptom_key].append([phrase,embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all post data from sqlite DB\n",
    "datadir = r\"C:\\Users\\keatu\\Regis_archive\\practicum_data\"\n",
    "dbfile = os.path.join(datadir,\"Facebook.db\")\n",
    "con = sqlite3.connect(dbfile)\n",
    "posts = pd.read_sql(\"select * from posts\",con)\n",
    "comments = pd.read_sql(\"select * from comments\", con)\n",
    "replies = pd.read_sql(\"Select * from replies\",con)\n",
    "con.close()\n",
    "\n",
    "# grab text/id fields from each data type--treating them all like unique posts\n",
    "all_text = pd.concat([\n",
    "                    posts[[\"user_id\",\"post_id\",\"text\"]],\n",
    "                    comments[[\"commenter_id\",\"comment_id\",\"comment_text\"]].rename(columns={\"commenter_id\":\"user_id\",\"comment_id\":\"post_id\",\"comment_text\":\"text\"}),\n",
    "                    replies[[\"commenter_id\",\"comment_id\",\"comment_text\"]].rename(columns={\"commenter_id\":\"user_id\",\"comment_id\":\"post_id\",\"comment_text\":\"text\"})\n",
    "                    ], sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195682"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare input sentence embedding to all matching sentence combinations\n",
    "# and use threshold cosine similarity score to indentify matches\n",
    "self_report_df = pd.DataFrame()\n",
    "threshold = 0.75 # cosine similarity threshold\n",
    "i=0\n",
    "for idx, row in all_text.iterrows():\n",
    "    i=+1\n",
    "    if (i%10000)==0:\n",
    "        print(\"{} completed of {}\".format(i,len(all_text)))\n",
    "    for sent in sent_tokenize(row['text']):\n",
    "        sent_embed = model.encode(sent, convert_to_tensor=True)\n",
    "        for term in symptom_embedding_map:\n",
    "            top_score = 0\n",
    "            top_match = \"\"\n",
    "            for (match_sentence,match_embedding) in symptom_embedding_map[term]:\n",
    "                cos_score = util.cos_sim(sent_embed, match_embedding).item()\n",
    "                if cos_score > top_score:\n",
    "                    top_score = cos_score\n",
    "                    top_match = match_sentence\n",
    "            if top_score > threshold:\n",
    "                self_report_df = self_report_df.append({'user_id':row['user_id'],\"post_id\":row['post_id'],'sentence':sent,\"match_sentence\":top_match,\"symptom\":term,\"cos_similarity\":top_score}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates using post_id field\n",
    "self_reportdf = self_report_df.groupby([\"post_id\"]).aggregate(\"first\").reset_index().drop(columns=\"Unnamed: 0\").sort_values(\"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab all unique user_ids from this subset of self-reporting individuals\n",
    "user_ids = self_reportdf[\"user_id\"].unique().tolist()\n",
    "user_ids = [str(i) for i in user_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get subset of posts, comments, and replies for only these users\n",
    "sr_posts = posts[posts[\"user_id\"].isin(user_ids)]\n",
    "sr_comments = comments[comments[\"commenter_id\"].isin(user_ids)]\n",
    "sr_replies = replies[replies[\"commenter_id\"].isin(user_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new database containing posts only for users self-reporting symptoms of interest\n",
    "# this database has an additional table for the self-reporting symptoms of interest entries\n",
    "outcon = sqlite3.connect(r\"C:\\Users\\keatu\\Regis_archive\\practicum_data\\Facebook_Self_Report.db\")\n",
    "sr_posts.astype(str).to_sql(\"posts\",con=outcon)\n",
    "sr_comments.astype(str).to_sql(\"comments\",con=outcon)\n",
    "sr_replies.astype(str).to_sql(\"replies\",con=outcon)\n",
    "self_reportdf.astype(str).to_sql(\"self_reporting\",con=outcon)\n",
    "outcon.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce4da9915f0d678e12335d58837d25529fa567b369eed2b2f7083145c2fca737"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
