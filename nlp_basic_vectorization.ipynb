{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sasdfsdf'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"sasdf.#$%#@$sd3f\"\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))  #remove punctuation\n",
    "nums = r'[0-9]'\n",
    "re.sub(nums, '',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "nums = r'[0-9]'\n",
    "stop_words = list(stopwords.words(\"english\"))\n",
    "def pre_process(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(nums, '',text) # remove numbers\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  #remove punctuation\n",
    "    text = [word for word in text.split() if word.lower() not in stop_words]  #remove stopwords\n",
    "    words = \"\"\n",
    "    for i in text:  #word stemming\n",
    "            words += (stemmer.stem(i))+\" \"\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data from sqlite DB\n",
    "datadir = r\"C:\\Users\\keatu\\Regis_archive\\practicum_data\"\n",
    "dbfile = os.path.join(datadir,\"Facebook.db\")\n",
    "con = sqlite3.connect(dbfile)\n",
    "posts = pd.read_sql(\"select * from posts\",con)\n",
    "comments = pd.read_sql(\"select * from comments\", con)\n",
    "replies = pd.read_sql(\"Select * from replies\",con)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total posts: 3815\n",
      "Total comments: 69326\n",
      "Total comment replies: 123641\n"
     ]
    }
   ],
   "source": [
    "print(\"Total posts: {}\".format(len(posts)))\n",
    "print(\"Total comments: {}\".format(len(comments)))\n",
    "print(\"Total comment replies: {}\".format(len(replies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all dataframes using only text and user id fields\n",
    "all_text = pd.concat([\n",
    "                    posts[[\"user_id\",\"post_id\",\"text\"]],\n",
    "                    comments[[\"commenter_id\",\"comment_id\",\"comment_text\"]].rename(columns={\"commenter_id\":\"user_id\",\"comment_text\":\"text\"}),\n",
    "                    replies[[\"commenter_id\",\"comment_id\",\"comment_text\"]].rename(columns={\"commenter_id\":\"user_id\",\"comment_text\":\"text\"})\n",
    "                    ], sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique users: 22586\n"
     ]
    }
   ],
   "source": [
    "print(\"Total unique users: {}\".format(all_text[\"user_id\"].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = all_text[\"text\"].apply(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the most frequently occuring words that are most meaningful from out text corpus\n",
    "\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=False)\n",
    "    \n",
    "    #get the feature names and tf-idf score of top n items\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create TfidVectorizer to create the numerical values\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\",decode_error='ignore', lowercase = True, min_df=2)\n",
    "\n",
    "#numericalize the textFeatures\n",
    "features = vectorizer.fit_transform(text_features.values.astype('U'))\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "#print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'covid': 0.001, 'long': 0.002, 'work': 0.001, 'like': 0.001, 'good': 0.002, 'sinc': 0.002, 'post': 0.002, 'bodi': 0.002, 'week': 0.002, 'month': 0.002, 'come': 0.002, 'posit': 0.002, 'mani': 0.002}\n"
     ]
    }
   ],
   "source": [
    "sorted_items=sort_coo(features.tocoo())\n",
    "\n",
    "keywords=extract_topn_from_vector(feature_names,sorted_items,15)\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = [\"blood clot\",\"heart\",\"cardiovascular\",\"stroke\",\"deep vein thrombosis\",\"embolism\",\"out of breath\",\"shortness of breath\",\"heparin\",\"warfarin\",\"rapid heartbeat\",\"heart rate\",\"lightheaded\",\"sweat\",\"fever\",\"leg pain\",\"leg swelling\", \"leg swollen\",\"clammy skin\",\"discolor skin\",\"cyanosis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heart-rat'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"manifest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce4da9915f0d678e12335d58837d25529fa567b369eed2b2f7083145c2fca737"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
