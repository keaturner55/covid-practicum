{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Overview: Facebook Scraper\n",
    "\n",
    "This notebook was used for the time-consuming data scraping. All of the heavy lifting is done by the \"facebook_scraper\" module (https://github.com/kevinzg/facebook-scraper) and the \"get_posts\" function from it. The tasks performed here are\n",
    "- Provide input Facebook pages to the \"get_posts\" function with appropriate options\n",
    "- Keep this process in an open while loop\n",
    "- Save post results to several timestamped csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "from facebook_scraper import get_posts, set_cookies, exceptions\n",
    "import time\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a function from the facebook-scraper module\n",
    "the file can be of a number of formats, but I used a\n",
    "json document that requires \"c-user\" and \"xs\" from a facebook login\n",
    "github: https://github.com/kevinzg/facebook-scraper\n",
    "\"\"\"\n",
    "set_cookies(\"cookies.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function is used to keep track of \"pagination\" URL's which is essentially a\n",
    "bookmark of which specific url you are using, so if you have to start over or hit\n",
    "a snag in the process, the \"get_posts\" function has a reference to continue requesting from\n",
    "where you left off instead of just scraping the newest pages each time.\n",
    "\"\"\"\n",
    "def handle_pagination_url(url):\n",
    "    global start_url\n",
    "    start_url = url\n",
    "\n",
    "start_url = None\n",
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of the group IDs I used for scraping (didn't include the last one)\n",
    "group_ids = [\"373920943948661\",\"365867864454134\",\"5950528321639271\",\"706985770086209\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-2-8_22-35-17 ## Sleeping 2 hrs\n",
      "2022-2-9_0-39-11 ## 1800 posts processed\n",
      "2022-2-9_0-57-26 ## 1900 posts processed\n",
      "2022-2-9_1-12-44 ## 2000 posts processed\n",
      "2022-2-9_1-18-40 ## TEMPORARY BAN at 2087 posts... sleeping for 1 hour\n",
      "2022-2-9_1-18-40 ## File written with timestamp\n",
      "2022-2-9_2-18-45 ## TEMPORARY BAN at 2087 posts... sleeping for 1 hour\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTemporarilyBanned\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-994654d22330>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         for post in get_posts(group = group_ids[0], page_limit = None, pages = 1000, start_url = start_url, encoding=\"utf-8\",\n\u001b[1;32m----> 9\u001b[1;33m         request_url_callback = handle_pagination_url, options={\"posts_per_page\":1000,\"allow_extra_requests\":True,\"comments\":True}):\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0mi\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\facebook_scraper\\facebook_scraper.py\u001b[0m in \u001b[0;36m_generic_get_posts\u001b[1;34m(self, extract_post_fn, iter_pages_fn, page_limit, options, remove_source, latest_date, max_past_limit, **kwargs)\u001b[0m\n\u001b[0;32m    935\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Starting to iterate pages\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 936\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miter_pages_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    937\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Extracting posts from page %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\facebook_scraper\\page_iterators.py\u001b[0m in \u001b[0;36mgeneric_iter_pages\u001b[1;34m(start_url, page_parser_cls, request_fn, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Requesting page from: %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m                 \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequest_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\facebook_scraper\\facebook_scraper.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    744\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_ban_titles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 745\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTemporarilyBanned\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    746\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[1;34m\">your account has been disabled<\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTemporarilyBanned\u001b[0m: Youâ€™re Temporarily Blocked",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-994654d22330>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{} ## File written with timestamp\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnowtime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3600\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# this is the primary loop that iterates through each post in a \"get_posts\" request that makes multiple requests\n",
    "results = []\n",
    "now = datetime.now()\n",
    "nowtime = \"{}-{}-{}_{}-{}-{}\".format(now.year,now.month,now.day,now.hour,now.minute,now.second)\n",
    "while True:\n",
    "    try :\n",
    "        for post in get_posts(group = group_ids[0], page_limit = None, pages = 1000, start_url = start_url, encoding=\"utf-8\",\n",
    "        request_url_callback = handle_pagination_url, options={\"posts_per_page\":1000,\"allow_extra_requests\":True,\"comments\":True}):\n",
    "            i+=1\n",
    "            # print a message very 100 posts\n",
    "            if (i%100)==0:\n",
    "                now = datetime.now()\n",
    "                nowtime = \"{}-{}-{}_{}-{}-{}\".format(now.year,now.month,now.day,now.hour,now.minute,now.second)\n",
    "                print(\"{} ## {} posts processed\".format(nowtime, i))\n",
    "            results.append(post)\n",
    "        # if the loop finishes--save results in a dataframe\n",
    "        all_posts = pd.DataFrame()\n",
    "        for each in results:\n",
    "            all_posts = all_posts.append(each, ignore_index = True)\n",
    "        \n",
    "        # export to csv with timestamp\n",
    "        if len(all_posts)>0:\n",
    "            all_posts.to_csv(\"fb_scraper_{}.csv\".format(nowtime))\n",
    "            print(\"{} ## File written with timestamp\".format(nowtime))\n",
    "        print(\"Finished\")\n",
    "        break\n",
    "    # temporary bans are common, so I would just save the results to a csv file\n",
    "    # when this happened, let the function sleep for about an hour, then keep trying\n",
    "    except exceptions.TemporarilyBanned:\n",
    "        now = datetime.now()\n",
    "        nowtime = \"{}-{}-{}_{}-{}-{}\".format(now.year,now.month,now.day,now.hour,now.minute,now.second)\n",
    "        print(\"{} ## TEMPORARY BAN at {} posts... sleeping for 1 hour\".format(nowtime,i))\n",
    "        all_posts = pd.DataFrame()\n",
    "        for each in results:\n",
    "            all_posts = all_posts.append(each, ignore_index = True)\n",
    "        if len(all_posts)>0:\n",
    "            all_posts.to_csv(\"fb_scraper_{}.csv\".format(nowtime))\n",
    "            print(\"{} ## File written with timestamp\".format(nowtime))\n",
    "        results = []\n",
    "        time.sleep(3600)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce4da9915f0d678e12335d58837d25529fa567b369eed2b2f7083145c2fca737"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
