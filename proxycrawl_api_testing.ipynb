{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Web Scraping Investigation\n",
    "\n",
    "The real challenge is finding COVID-positive people and then following those users to see cardiovascular-related symptoms in them... Existing studies have used the term covid long-haulers and social media pages associated with that to identify people who have had it and create a timeline for those people and tie symptoms to that timeline. This project seems a bit more challenging, because we are not only interested with positive-covid patients, but we are narrowing down the pool of people to those who have reported soecific cardiovascular events following this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proxycrawl.proxycrawl_api import ProxyCrawlAPI\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_token = \"Gzb-YK7SoijLHCDpqKMc0g\"\n",
    "normal_token = \"H-EOaYFwXN76Wg4C6VvLrA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawling function\n",
    "def crawl(token,url,scraper,scroll=False,scroll_interval = 0):\n",
    "    api = ProxyCrawlAPI({'token': token})\n",
    "    if scroll:\n",
    "        response = api.get(url,{\n",
    "            'scraper':scraper, 'scroll':'true','scroll_interval':scroll_interval\n",
    "        })\n",
    "    else:\n",
    "        response = api.get(url, {\n",
    "            'scraper':scraper\n",
    "    })\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facebook\n",
    "The hard part with facebook is getting past permissions... The best way to do this (in my opinion) is to look at specific groups of people (ideally groups like COVID support groups or long-hauler groups). FB seemed like a good option, but I can't get any useable data from the facebook group api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520\n"
     ]
    }
   ],
   "source": [
    "#while response['status_code'] != 200:\n",
    "api = ProxyCrawlAPI({'token': js_token})\n",
    "response = api.get('https://www.facebook.com/Amazon/', {\n",
    "    'scraper':'facebook-page', 'scroll':'true','scroll_interval':15\n",
    "})\n",
    "print(response['status_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "response = crawl(js_token,'https://www.facebook.com/groups/373920943948661','facbook-group', True, 58)\n",
    "print(response['status_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('facebook-generic.json','wb+') as f:\n",
    "    f.write(response['body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instagram-Hashtag\n",
    "Targeting specific groups/hashtags seems to be the best way of getting positive diagnosis information. Long-haulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520\n"
     ]
    }
   ],
   "source": [
    "api = ProxyCrawlAPI({'token': js_token})\n",
    "response = api.get('https://www.instagram.com/explore/tags/longhaulers/', {\n",
    "    'scraper':'instagram-hashtag','scroll':'true','scroll_interval': 10\n",
    "})\n",
    "print(response['status_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('instagram-hashtag-longhaulers3-js.json', 'wb+') as f:\n",
    "    f.write(response['body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quora-serp\n",
    "Initial idea here is to find users asking questions about symptoms after covid--this could possibly lend itself well to finding individuals who have contracted covid and you could include terms from the symptom list in the search queries.\n",
    "\n",
    "Down the line--there is also a scraper extension that can dig into the resonses from these questions (quora-question)... Could be a good idea to possibly create recursive searches and data subsets driven by covid-positive individuals asking questions of interest (i.e. questions pertaining to cardiovascular issues w/ symptoms from our symptom list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl(token,url,scraper,scroll=False,scroll_interval = 0):\n",
    "    api = ProxyCrawlAPI({'token': token})\n",
    "    if scroll:\n",
    "        response = api.get(url,{\n",
    "            'scraper':scraper, 'scroll':'true','scroll_interval':scroll_interval\n",
    "        })\n",
    "    else:\n",
    "        response = api.get(url, {\n",
    "            'scraper':scraper\n",
    "    })\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "response = crawl(js_token,'https://www.quora.com/search?q=heart%20problems%20after%20covid','quora-serp',True,60)\n",
    "#api = ProxyCrawlAPI({'token': js_token})\n",
    "#response = api.get('https://www.quora.com/search?q=heart%20problems%20after%20covid', {\n",
    "#    'scraper':'quora-serp','scroll':'true','scroll_interval':60\n",
    "#})\n",
    "print(response['status_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(response['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching stroke\n",
      "stroke failed with status 520\n",
      "Searching deep vein thrombosis\n",
      "The read operation timed out\n",
      "Searching embolism\n",
      "Searching breathing\n",
      "Searching heparin\n",
      "Searching warfarin\n",
      "Searching rapid heartbeat\n",
      "Searching lightheaded\n",
      "Searching sweating\n",
      "Searching fever\n",
      "Searching leg pain\n",
      "The read operation timed out\n",
      "Searching leg swelling\n",
      "Searching leg swollen\n",
      "leg swollen failed with status 520\n",
      "Searching clammy skin\n",
      "Searching discolor skin\n",
      "Searching cyanosis\n",
      "cyanosis failed with status 520\n"
     ]
    }
   ],
   "source": [
    "search_terms = [\"blood clot\",\"heart\",\"cardiovascular\",\"stroke\",\"deep vein thrombosis\",\"embolism\",\"breathing\",\"heparin\",\"warfarin\",\"rapid heartbeat\",\"lightheaded\",\"sweating\",\"fever\",\"leg pain\",\"leg swelling\", \"leg swollen\",\"clammy skin\",\"discolor skin\",\"cyanosis\"]\n",
    "base_url = \"https://www.quora.com/search?q={}%20after%20covid\"\n",
    "for term in search_terms[3:]:\n",
    "    print(\"Searching {}\".format(term))\n",
    "    term_no_space = term.replace(\" \",\"%20\")\n",
    "    url = base_url.format(term_no_space)\n",
    "    try:\n",
    "        response = crawl(js_token,url,'quora-serp',True,60)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    if response['status_code'] == 200:\n",
    "        with open('quora-serp-{}.json'.format(term.replace(\" \",\"_\")), 'wb+') as f:\n",
    "            f.write(response['body'])\n",
    "    else:\n",
    "        print(\"{} failed with status {}\".format(term,response['status_code']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"'https://www.quora.com/search?q={}%20after%20covid\".format(\"blood%20clot\")\n",
    "response = crawl(js_token,'https://www.quora.com/search?q=heart%20problems%20after%20covid','quora-serp',True,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce4da9915f0d678e12335d58837d25529fa567b369eed2b2f7083145c2fca737"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
